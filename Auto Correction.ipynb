{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ns3yqOsHm-o5",
    "outputId": "01e3551e-e528-43ac-8bc2-d46e47e2f7ec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pattern\n",
      "  Downloading Pattern-3.6.0.tar.gz (22.2 MB)\n",
      "     ---------------------------------------- 22.2/22.2 MB 3.6 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: future in f:\\users\\nikki rani\\lib\\site-packages (from pattern) (0.18.2)\n",
      "Collecting backports.csv\n",
      "  Downloading backports.csv-1.0.7-py2.py3-none-any.whl (12 kB)\n",
      "Collecting mysqlclient\n",
      "  Downloading mysqlclient-2.1.1-cp38-cp38-win_amd64.whl (178 kB)\n",
      "     -------------------------------------- 178.6/178.6 kB 2.7 MB/s eta 0:00:00\n",
      "Requirement already satisfied: beautifulsoup4 in f:\\users\\nikki rani\\lib\\site-packages (from pattern) (4.9.3)\n",
      "Requirement already satisfied: lxml in f:\\users\\nikki rani\\lib\\site-packages (from pattern) (4.6.3)\n",
      "Collecting feedparser\n",
      "  Downloading feedparser-6.0.10-py3-none-any.whl (81 kB)\n",
      "     ---------------------------------------- 81.1/81.1 kB 4.7 MB/s eta 0:00:00\n",
      "Collecting pdfminer.six\n",
      "  Downloading pdfminer.six-20220524-py3-none-any.whl (5.6 MB)\n",
      "     ---------------------------------------- 5.6/5.6 MB 3.9 MB/s eta 0:00:00\n",
      "Requirement already satisfied: numpy in f:\\users\\nikki rani\\lib\\site-packages (from pattern) (1.20.1)\n",
      "Requirement already satisfied: scipy in f:\\users\\nikki rani\\lib\\site-packages (from pattern) (1.6.2)\n",
      "Requirement already satisfied: nltk in f:\\users\\nikki rani\\lib\\site-packages (from pattern) (3.6.1)\n",
      "Collecting python-docx\n",
      "  Downloading python-docx-0.8.11.tar.gz (5.6 MB)\n",
      "     ---------------------------------------- 5.6/5.6 MB 2.4 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting cherrypy\n",
      "  Downloading CherryPy-18.8.0-py2.py3-none-any.whl (348 kB)\n",
      "     -------------------------------------- 348.4/348.4 kB 1.0 MB/s eta 0:00:00\n",
      "Requirement already satisfied: requests in f:\\users\\nikki rani\\lib\\site-packages (from pattern) (2.25.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in f:\\users\\nikki rani\\lib\\site-packages (from beautifulsoup4->pattern) (2.2.1)\n",
      "Collecting zc.lockfile\n",
      "  Downloading zc.lockfile-2.0-py2.py3-none-any.whl (9.7 kB)\n",
      "Collecting cheroot>=8.2.1\n",
      "  Downloading cheroot-8.6.0-py2.py3-none-any.whl (104 kB)\n",
      "     -------------------------------------- 104.7/104.7 kB 1.2 MB/s eta 0:00:00\n",
      "Requirement already satisfied: more-itertools in f:\\users\\nikki rani\\lib\\site-packages (from cherrypy->pattern) (8.7.0)\n",
      "Collecting portend>=2.1.1\n",
      "  Downloading portend-3.1.0-py3-none-any.whl (5.3 kB)\n",
      "Requirement already satisfied: pywin32>=227 in f:\\users\\nikki rani\\lib\\site-packages (from cherrypy->pattern) (227)\n",
      "Collecting jaraco.collections\n",
      "  Downloading jaraco.collections-3.5.2-py3-none-any.whl (10 kB)\n",
      "Collecting sgmllib3k\n",
      "  Downloading sgmllib3k-1.0.0.tar.gz (5.8 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: tqdm in f:\\users\\nikki rani\\lib\\site-packages (from nltk->pattern) (4.59.0)\n",
      "Requirement already satisfied: click in f:\\users\\nikki rani\\lib\\site-packages (from nltk->pattern) (7.1.2)\n",
      "Requirement already satisfied: joblib in f:\\users\\nikki rani\\lib\\site-packages (from nltk->pattern) (1.0.1)\n",
      "Requirement already satisfied: regex in f:\\users\\nikki rani\\lib\\site-packages (from nltk->pattern) (2021.4.4)\n",
      "Collecting cryptography>=36.0.0\n",
      "  Downloading cryptography-38.0.1-cp36-abi3-win_amd64.whl (2.4 MB)\n",
      "     ---------------------------------------- 2.4/2.4 MB 3.3 MB/s eta 0:00:00\n",
      "Collecting charset-normalizer>=2.0.0\n",
      "  Downloading charset_normalizer-2.1.1-py3-none-any.whl (39 kB)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in f:\\users\\nikki rani\\lib\\site-packages (from requests->pattern) (2020.12.5)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in f:\\users\\nikki rani\\lib\\site-packages (from requests->pattern) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in f:\\users\\nikki rani\\lib\\site-packages (from requests->pattern) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in f:\\users\\nikki rani\\lib\\site-packages (from requests->pattern) (1.26.4)\n",
      "Requirement already satisfied: six>=1.11.0 in f:\\users\\nikki rani\\lib\\site-packages (from cheroot>=8.2.1->cherrypy->pattern) (1.15.0)\n",
      "Collecting jaraco.functools\n",
      "  Downloading jaraco.functools-3.5.2-py3-none-any.whl (7.3 kB)\n",
      "Requirement already satisfied: cffi>=1.12 in f:\\users\\nikki rani\\lib\\site-packages (from cryptography>=36.0.0->pdfminer.six->pattern) (1.14.5)\n",
      "Collecting tempora>=1.8\n",
      "  Downloading tempora-5.0.2-py3-none-any.whl (15 kB)\n",
      "Collecting jaraco.text\n",
      "  Downloading jaraco.text-3.9.1-py3-none-any.whl (10 kB)\n",
      "Collecting jaraco.classes\n",
      "  Downloading jaraco.classes-3.2.3-py3-none-any.whl (6.0 kB)\n",
      "Requirement already satisfied: setuptools in f:\\users\\nikki rani\\lib\\site-packages (from zc.lockfile->cherrypy->pattern) (52.0.0.post20210125)\n",
      "Requirement already satisfied: pycparser in f:\\users\\nikki rani\\lib\\site-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six->pattern) (2.20)\n",
      "Requirement already satisfied: pytz in f:\\users\\nikki rani\\lib\\site-packages (from tempora>=1.8->portend>=2.1.1->cherrypy->pattern) (2021.1)\n",
      "Collecting autocommand\n",
      "  Downloading autocommand-2.2.1-py3-none-any.whl (22 kB)\n",
      "Collecting inflect\n",
      "  Downloading inflect-6.0.0-py3-none-any.whl (34 kB)\n",
      "Collecting importlib-resources\n",
      "  Downloading importlib_resources-5.10.0-py3-none-any.whl (34 kB)\n",
      "Collecting jaraco.context>=4.1\n",
      "  Downloading jaraco.context-4.1.2-py3-none-any.whl (4.7 kB)\n",
      "Requirement already satisfied: zipp>=3.1.0 in f:\\users\\nikki rani\\lib\\site-packages (from importlib-resources->jaraco.text->jaraco.collections->cherrypy->pattern) (3.4.1)\n",
      "Collecting pydantic\n",
      "  Downloading pydantic-1.10.2-cp38-cp38-win_amd64.whl (2.2 MB)\n",
      "     ---------------------------------------- 2.2/2.2 MB 3.8 MB/s eta 0:00:00\n",
      "Collecting typing-extensions>=4.1.0\n",
      "  Downloading typing_extensions-4.4.0-py3-none-any.whl (26 kB)\n",
      "Building wheels for collected packages: pattern, python-docx, sgmllib3k\n",
      "  Building wheel for pattern (setup.py): started\n",
      "  Building wheel for pattern (setup.py): finished with status 'done'\n",
      "  Created wheel for pattern: filename=Pattern-3.6-py3-none-any.whl size=22332723 sha256=afbaabeb9218bc4d5625f9d8adf1c9ee0a73687fe17768e991eb0a7c52515364\n",
      "  Stored in directory: c:\\users\\nikki rani\\appdata\\local\\pip\\cache\\wheels\\ec\\ce\\8f\\bccc2d04f3a25a5a1dd19165b2855ad3203975f25edd5838d6\n",
      "  Building wheel for python-docx (setup.py): started\n",
      "  Building wheel for python-docx (setup.py): finished with status 'done'\n",
      "  Created wheel for python-docx: filename=python_docx-0.8.11-py3-none-any.whl size=184600 sha256=d971cd3f33b51097d50db25cf940a1591c7778f86db1bc2f7654e2df6e42715b\n",
      "  Stored in directory: c:\\users\\nikki rani\\appdata\\local\\pip\\cache\\wheels\\32\\b8\\b2\\c4c2b95765e615fe139b0b17b5ea7c0e1b6519b0a9ec8fb34d\n",
      "  Building wheel for sgmllib3k (setup.py): started\n",
      "  Building wheel for sgmllib3k (setup.py): finished with status 'done'\n",
      "  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-py3-none-any.whl size=6065 sha256=f430825204937a35f8ab32e51e16c9e8e1d26e8ad8dacc36faed5c52681950ba\n",
      "  Stored in directory: c:\\users\\nikki rani\\appdata\\local\\pip\\cache\\wheels\\83\\63\\2f\\117884c3b19d46b64d3d61690333aa80c88dc14050e269c546\n",
      "Successfully built pattern python-docx sgmllib3k\n",
      "Installing collected packages: sgmllib3k, backports.csv, autocommand, zc.lockfile, typing-extensions, python-docx, mysqlclient, jaraco.functools, jaraco.context, jaraco.classes, importlib-resources, feedparser, charset-normalizer, tempora, pydantic, cryptography, cheroot, portend, pdfminer.six, inflect, jaraco.text, jaraco.collections, cherrypy, pattern\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing-extensions 3.7.4.3\n",
      "    Uninstalling typing-extensions-3.7.4.3:\n",
      "      Successfully uninstalled typing-extensions-3.7.4.3\n",
      "  Attempting uninstall: cryptography\n",
      "    Found existing installation: cryptography 3.4.7\n",
      "    Uninstalling cryptography-3.4.7:\n",
      "      Successfully uninstalled cryptography-3.4.7\n",
      "Successfully installed autocommand-2.2.1 backports.csv-1.0.7 charset-normalizer-2.1.1 cheroot-8.6.0 cherrypy-18.8.0 cryptography-38.0.1 feedparser-6.0.10 importlib-resources-5.10.0 inflect-6.0.0 jaraco.classes-3.2.3 jaraco.collections-3.5.2 jaraco.context-4.1.2 jaraco.functools-3.5.2 jaraco.text-3.9.1 mysqlclient-2.1.1 pattern-3.6 pdfminer.six-20220524 portend-3.1.0 pydantic-1.10.2 python-docx-0.8.11 sgmllib3k-1.0.0 tempora-5.0.2 typing-extensions-4.4.0 zc.lockfile-2.0\n",
      "\n",
      "[notice] A new release of pip available: 22.1.2 -> 22.2.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "Requirement already satisfied: pyspellchecker in f:\\users\\nikki rani\\lib\\site-packages (0.7.0)\n",
      "\n",
      "[notice] A new release of pip available: 22.1.2 -> 22.2.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "Requirement already satisfied: autocorrect in f:\\users\\nikki rani\\lib\\site-packages (2.6.1)\n",
      "\n",
      "[notice] A new release of pip available: 22.1.2 -> 22.2.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "Requirement already satisfied: textblob in f:\\users\\nikki rani\\lib\\site-packages (0.17.1)\n",
      "Requirement already satisfied: nltk>=3.1 in f:\\users\\nikki rani\\lib\\site-packages (from textblob) (3.6.1)\n",
      "Requirement already satisfied: regex in f:\\users\\nikki rani\\lib\\site-packages (from nltk>=3.1->textblob) (2021.4.4)\n",
      "Requirement already satisfied: joblib in f:\\users\\nikki rani\\lib\\site-packages (from nltk>=3.1->textblob) (1.0.1)\n",
      "Requirement already satisfied: click in f:\\users\\nikki rani\\lib\\site-packages (from nltk>=3.1->textblob) (7.1.2)\n",
      "Requirement already satisfied: tqdm in f:\\users\\nikki rani\\lib\\site-packages (from nltk>=3.1->textblob) (4.59.0)\n",
      "\n",
      "[notice] A new release of pip available: 22.1.2 -> 22.2.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "Requirement already satisfied: textdistance in f:\\users\\nikki rani\\lib\\site-packages (4.2.1)\n",
      "\n",
      "[notice] A new release of pip available: 22.1.2 -> 22.2.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# installing libraries\n",
    "\n",
    "# used for text processing and data mining\n",
    "!pip install pattern\n",
    "\n",
    "!pip install pyspellchecker \n",
    "!pip install autocorrect\n",
    "\n",
    "# It provides a simple API for diving into common natural language processing (NLP) tasks such as part-of-speech tagging, \n",
    "# noun phrase extraction, sentiment analysis, classification, translation, and more.\n",
    "!pip install textblob\n",
    "\n",
    "# for comparing distance between two or more sequences\n",
    "!pip install textdistance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OhP8cAJpnMo6",
    "outputId": "e1e7db09-687a-47a4-c868-3a8e3a1527c8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first 10 words in our dictionary are: \n",
      "['a', 'ability', 'able', 'about', 'above', 'accept', 'according', 'account', 'across', 'act']\n",
      "The dictionary has 1001 words \n"
     ]
    }
   ],
   "source": [
    "# Step 1: Data Preprocessing\n",
    "import re  # regular expression\n",
    "from collections import Counter # counter is a dict subclass for counting hashable objects\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Implement the function process_data which\n",
    "# 1) Reads in a corpus\n",
    "# 2) Changes everything to lowercase\n",
    "# 3) Returns a list of words\n",
    "\n",
    "w = [] #words\n",
    "with open('sample.txt','r',encoding=\"utf8\") as f:\n",
    "    file_name_data = f.read()\n",
    "    file_name_data = file_name_data.lower()\n",
    "    w = re.findall('\\w+', file_name_data)\n",
    "\n",
    "v = set(w) #vocabulary\n",
    "print(f\"The first 10 words in our dictionary are: \\n{w[0:10]}\")\n",
    "print(f\"The dictionary has {len(v)} words \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dictionary has  1001 key values pairs\n"
     ]
    }
   ],
   "source": [
    "# a get_count function that returns a dictionary of word vs frequency\n",
    "def get_count(words):\n",
    "    word_count = {}\n",
    "    for word in words:\n",
    "        if word in word_count:\n",
    "            word_count[word] += 1\n",
    "        else:\n",
    "            word_count[word] = 1\n",
    "    return word_count\n",
    "\n",
    "\n",
    "word_count = get_count(w)\n",
    "print(f\"The dictionary has  {len(word_count)} key values pairs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "2Gpd-mt3oAix"
   },
   "outputs": [],
   "source": [
    "# implement get_probs function\n",
    "# to calculate the probability that any word will appear if randomly selected from the dictionary\n",
    "\n",
    "def get_probs(word_count_dict):\n",
    "    probs = {}\n",
    "    m = sum(word_count_dict.values())\n",
    "    for key in word_count_dict.keys():\n",
    "        probs[key] = word_count_dict[key] / m\n",
    "    return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RZNZ7o3xoK7H",
    "outputId": "e5455797-3597-436d-ee39-107abd1937c9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ans', 'cns', 'cas', 'can']\n"
     ]
    }
   ],
   "source": [
    "# Now we implement 4 edit word functions\n",
    "\n",
    "# DeleteLetter - removes a letter from a given word\n",
    "def DeleteLetter(word):\n",
    "    delete_list = []\n",
    "    split_list = []\n",
    "    for i in range(len(word)):\n",
    "        split_list.append((word[0:i], word[i:]))\n",
    "    for a, b in split_list:\n",
    "        delete_list.append(a + b[1:])\n",
    "    return delete_list\n",
    "\n",
    "delete_word_l = DeleteLetter(word=\"cans\")\n",
    "print(delete_word_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Bqz0sTDIoOVe",
    "outputId": "bff2aaf0-0273-4a6b-e2cd-3c847812163a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['rash', 'tash', 'trsh', 'trah', 'tras']\n"
     ]
    }
   ],
   "source": [
    "print(DeleteLetter(\"trash\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xDrdPsh5KhFl",
    "outputId": "3c989faa-3bba-4629-dbd0-dde1c5005024"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ans', 'cns', 'cas', 'can']\n"
     ]
    }
   ],
   "source": [
    "print(DeleteLetter(\"cans\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MfjFMbYwoQ96",
    "outputId": "b14cefcb-1f98-479e-c3c0-d533a9090e49"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tea', 'eat']\n"
     ]
    }
   ],
   "source": [
    "# SwitchLetter - swap two adjacent letters\n",
    "def SwitchLetter(word):\n",
    "    split_l = []\n",
    "    switch_l = []\n",
    "    for i in range(len(word)):\n",
    "        split_l.append((word[0:i], word[i:]))\n",
    "    switch_l = [a + b[1] + b[0] + b[2:] for a, b in split_l if len(b) >= 2]\n",
    "    return switch_l\n",
    "\n",
    "switch_word_l = SwitchLetter(word=\"eta\")\n",
    "print(switch_word_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0Bg6uBMyoTWo",
    "outputId": "27f51149-d7f2-4bbd-91dc-5d2733b014bb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['rtash', 'tarsh', 'trsah', 'trahs']\n"
     ]
    }
   ],
   "source": [
    "print(SwitchLetter(\"trash\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "U1pBhBEtoVhX",
    "outputId": "e2b764e6-ad20-4d95-addc-69a991d939a6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['aan', 'ban', 'can', 'dan', 'ean', 'fan', 'gan', 'han', 'ian', 'jan', 'kan', 'lan', 'man', 'nan', 'oan', 'pan', 'qan', 'ran', 'san', 'tan', 'uan', 'van', 'wan', 'xan', 'yan', 'zan', 'can', 'cbn', 'ccn', 'cdn', 'cen', 'cfn', 'cgn', 'chn', 'cin', 'cjn', 'ckn', 'cln', 'cmn', 'cnn', 'con', 'cpn', 'cqn', 'crn', 'csn', 'ctn', 'cun', 'cvn', 'cwn', 'cxn', 'cyn', 'czn', 'caa', 'cab', 'cac', 'cad', 'cae', 'caf', 'cag', 'cah', 'cai', 'caj', 'cak', 'cal', 'cam', 'can', 'cao', 'cap', 'caq', 'car', 'cas', 'cat', 'cau', 'cav', 'caw', 'cax', 'cay', 'caz']\n"
     ]
    }
   ],
   "source": [
    "# replace_letter: changes one letter to another\n",
    "def replace_letter(word):\n",
    "    split_l = []\n",
    "    replace_list = []\n",
    "    for i in range(len(word)):\n",
    "        split_l.append((word[0:i], word[i:]))\n",
    "    alphabets = 'abcdefghijklmnopqrstuvwxyz'\n",
    "    replace_list = [a + l + (b[1:] if len(b) > 1 else '') for a, b in split_l if b for l in alphabets]\n",
    "    return replace_list\n",
    "\n",
    "replace_l = replace_letter(word='can')\n",
    "print(replace_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dI9TK9vuoXhr",
    "outputId": "4f42a420-b24d-4849-9bab-29743bdfd9ee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['arash', 'brash', 'crash', 'drash', 'erash', 'frash', 'grash', 'hrash', 'irash', 'jrash', 'krash', 'lrash', 'mrash', 'nrash', 'orash', 'prash', 'qrash', 'rrash', 'srash', 'trash', 'urash', 'vrash', 'wrash', 'xrash', 'yrash', 'zrash', 'taash', 'tbash', 'tcash', 'tdash', 'teash', 'tfash', 'tgash', 'thash', 'tiash', 'tjash', 'tkash', 'tlash', 'tmash', 'tnash', 'toash', 'tpash', 'tqash', 'trash', 'tsash', 'ttash', 'tuash', 'tvash', 'twash', 'txash', 'tyash', 'tzash', 'trash', 'trbsh', 'trcsh', 'trdsh', 'tresh', 'trfsh', 'trgsh', 'trhsh', 'trish', 'trjsh', 'trksh', 'trlsh', 'trmsh', 'trnsh', 'trosh', 'trpsh', 'trqsh', 'trrsh', 'trssh', 'trtsh', 'trush', 'trvsh', 'trwsh', 'trxsh', 'trysh', 'trzsh', 'traah', 'trabh', 'trach', 'tradh', 'traeh', 'trafh', 'tragh', 'trahh', 'traih', 'trajh', 'trakh', 'tralh', 'tramh', 'tranh', 'traoh', 'traph', 'traqh', 'trarh', 'trash', 'trath', 'trauh', 'travh', 'trawh', 'traxh', 'trayh', 'trazh', 'trasa', 'trasb', 'trasc', 'trasd', 'trase', 'trasf', 'trasg', 'trash', 'trasi', 'trasj', 'trask', 'trasl', 'trasm', 'trasn', 'traso', 'trasp', 'trasq', 'trasr', 'trass', 'trast', 'trasu', 'trasv', 'trasw', 'trasx', 'trasy', 'trasz']\n"
     ]
    }
   ],
   "source": [
    "print(replace_letter(\"trash\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "2amMFui4oZRE"
   },
   "outputs": [],
   "source": [
    "# insert_letter: adds additional characters\n",
    "def insert_letter(word):\n",
    "    split_l = []\n",
    "    insert_list = []\n",
    "    for i in range(len(word) + 1):\n",
    "        split_l.append((word[0:i], word[i:]))\n",
    "    letters = 'abcdefghijklmnopqrstuvwxyz'\n",
    "    insert_list = [a + l + b for a, b in split_l for l in letters]\n",
    "    # print(split_l)\n",
    "    return insert_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 252
    },
    "id": "v71yHIF2oc0a",
    "outputId": "a1fc4c25-4343-4a1b-bb86-c1bd66c659d0"
   },
   "outputs": [],
   "source": [
    "# combining the edits\n",
    "# switch operation optional\n",
    "def edit_one_letter(word, allow_switches=True):\n",
    "    edit_set1 = set()\n",
    "    edit_set1.update(DeleteLetter(word))\n",
    "    if allow_switches:\n",
    "        edit_set1.update(SwitchLetter(word))\n",
    "    edit_set1.update(replace_letter(word))\n",
    "    edit_set1.update(insert_letter(word))\n",
    "    return edit_set1\n",
    "\n",
    "# edit two letters\n",
    "def edit_two_letters(word, allow_switches=True):\n",
    "    edit_set2 = set()\n",
    "    edit_one = edit_one_letter(word, allow_switches=allow_switches)\n",
    "    for w in edit_one:\n",
    "        if w:\n",
    "            edit_two = edit_one_letter(w, allow_switches=allow_switches)\n",
    "            edit_set2.update(edit_two)\n",
    "    return edit_set2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "huRb8bGPomRX"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter any word:daed\n",
      "word 0: dead, probability 0.000999\n"
     ]
    }
   ],
   "source": [
    "# get corrected word\n",
    "def get_corrections(word, probs, vocab, n=2):\n",
    "    suggested_word = []\n",
    "    best_suggestion = []\n",
    "    suggested_word = list(\n",
    "        (word in vocab and word) or edit_one_letter(word).intersection(vocab) or edit_two_letters(word).intersection(\n",
    "            vocab))\n",
    "    best_suggestion = [[s, probs[s]] for s in list(reversed(suggested_word))]\n",
    "    return best_suggestion\n",
    "\n",
    "my_word = input(\"Enter any word:\")\n",
    "probs = get_probs(word_count)\n",
    "tmp_corrections = get_corrections(my_word, probs, v, 2)\n",
    "for i, word_prob in enumerate(tmp_corrections):\n",
    "    print(f\"word {i}: {word_prob[0]}, probability {word_prob[1]:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter any word:ant\n",
      "word 0: and, probability 0.000999\n",
      "word 1: want, probability 0.000999\n",
      "word 2: act, probability 0.000999\n",
      "word 3: art, probability 0.000999\n",
      "word 4: at, probability 0.000999\n",
      "word 5: any, probability 0.000999\n"
     ]
    }
   ],
   "source": [
    "my_word = input(\"Enter any word:\")\n",
    "probs = get_probs(word_count)\n",
    "tmp_corrections = get_corrections(my_word, probs, v, 2)\n",
    "for i, word_prob in enumerate(tmp_corrections):\n",
    "    print(f\"word {i}: {word_prob[0]}, probability {word_prob[1]:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
